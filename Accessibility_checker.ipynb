{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhF3XSzvIioq4DZnAWFRtL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "198c4b5d825b4227b2fe2b8bf01a568d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1c008cd960b41fd979f3d03ac334480",
              "IPY_MODEL_08c16194c4594c79865b7dcba079d7a2",
              "IPY_MODEL_9d41d53febcb414696c6238d970287b7"
            ],
            "layout": "IPY_MODEL_1a97cbfc71234e9297235d0dc42f0348"
          }
        },
        "a1c008cd960b41fd979f3d03ac334480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79eede3476a641378684dd68e01749cd",
            "placeholder": "​",
            "style": "IPY_MODEL_8d1f23f5b38b4040b77b71acc5018b62",
            "value": "unsloth.Q8_0.gguf: 100%"
          }
        },
        "08c16194c4594c79865b7dcba079d7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8b1193e26f34bb4b1a315f7a25de32e",
            "max": 8540770784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae1dbcb3fec5449eadcfe3de580f5243",
            "value": 8540770784
          }
        },
        "9d41d53febcb414696c6238d970287b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_457bcada385c488cb81b8988f296ec72",
            "placeholder": "​",
            "style": "IPY_MODEL_22ad31de617d44fb8a45582cfa7c9b7c",
            "value": " 8.54G/8.54G [03:23&lt;00:00, 42.5MB/s]"
          }
        },
        "1a97cbfc71234e9297235d0dc42f0348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79eede3476a641378684dd68e01749cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d1f23f5b38b4040b77b71acc5018b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8b1193e26f34bb4b1a315f7a25de32e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae1dbcb3fec5449eadcfe3de580f5243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "457bcada385c488cb81b8988f296ec72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ad31de617d44fb8a45582cfa7c9b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/it21312908/AI-ML-colab/blob/main/Accessibility_checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRrBCfSlYHTO",
        "outputId": "8690eb40-aa64-4268-f070-4aab2e913953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 41689, done.\u001b[K\n",
            "remote: Counting objects: 100% (9657/9657), done.\u001b[K\n",
            "remote: Compressing objects: 100% (549/549), done.\u001b[K\n",
            "remote: Total 41689 (delta 9378), reused 9108 (delta 9108), pack-reused 32032 (from 1)\u001b[K\n",
            "Receiving objects: 100% (41689/41689), 72.47 MiB | 11.98 MiB/s, done.\n",
            "Resolving deltas: 100% (30552/30552), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yUCxlTZYT-f",
        "outputId": "34e2412a-9a03-478b-9c6c-a6e04a285303"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -B build -DLLAMA_CURL=ON\n",
        "!cmake --build build -j --target llama-cli\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7oBv-VQYcpa",
        "outputId": "be59912f-f3f4-46da-8093-d2d333ce36e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (2.8s)\n",
            "-- Generating done (0.4s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  0%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 19%] Built target build_info\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX shared library libggml-base.so\u001b[0m\n",
            "[ 19%] Built target ggml-base\n",
            "[ 23%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX shared library libggml-cpu.so\u001b[0m\n",
            "[ 38%] Built target ggml-cpu\n",
            "[ 42%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX shared library libggml.so\u001b[0m\n",
            "[ 42%] Built target ggml\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX shared library libllama.so\u001b[0m\n",
            "[ 80%] Built target llama\n",
            "[ 80%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 96%] Built target common\n",
            "[ 96%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[100%] Built target llama-cli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli \\\n",
        "    --hf-repo \"DSGT6AITest/webAccessibility\" \\\n",
        "    --hf-file unsloth.Q8_0.gguf \\\n",
        "    -p \"Once upon a time,\" \\\n",
        "    --temp 0.7 \\\n",
        "    --top-k 40\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywm6kSbzhMvg",
        "outputId": "38950a37-c602-4765-cd05-edb0c5c19507"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4508 (a1649cc1) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "common_download_file: previous metadata file found /root/.cache/llama.cpp/DSGT6AITest_webAccessibility_unsloth.Q8_0.gguf.json: {\"etag\":\"\\\"d13298bb725d8115b2769df060dc34c7-534\\\"\",\"lastModified\":\"Wed, 20 Nov 2024 06:18:24 GMT\",\"url\":\"https://huggingface.co/DSGT6AITest/webAccessibility/resolve/main/unsloth.Q8_0.gguf\"}\n",
            "curl_perform_with_retry: Trying to download from https://huggingface.co/DSGT6AITest/webAccessibility/resolve/main/unsloth.Q8_0.gguf (attempt 1 of 3)...\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /root/.cache/llama.cpp/DSGT6AITest_webAccessibility_unsloth.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8b Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = meta-llama-3.1\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 7.95 GiB (8.50 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.7999 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 500000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 8B\n",
            "print_info: model params     = 8.03 B\n",
            "print_info: general.name     = Meta Llama 3.1 8b Bnb 4bit\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128001 '<|end_of_text|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: EOM token        = 128008 '<|eom_id|>'\n",
            "print_info: PAD token        = 128004 '<|finetune_right_pad_id|>'\n",
            "print_info: LF token         = 128 'Ä'\n",
            "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
            "print_info: EOG token        = 128008 '<|eom_id|>'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 2048\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 500000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1030\n",
            "llama_init_from_model: graph splits = 1\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "sampler seed: 3334331013\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.700\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "Once upon a time, there was a tiny girl named Dora. Dora was a very curious girl and she often got herself into trouble as a result of her curiosity. One day Dora was playing outside and she noticed a strange looking animal. The animal was small and black and had long sharp teeth. Dora decided to approach the animal and see if it was friendly.\n",
            "As Dora got closer to the animal she noticed that it was not friendly at all. The animal growled at her and tried to bite her. Dora ran away as fast as she could and told her mother about the encounter.\n",
            "Dora’s mother was very concerned about the incident and she decided to take action. She called the local wildlife authority and asked them to come and remove the animal from the area. The wildlife authority quickly responded and removed the animal from the area.\n",
            "Dora was very relieved that the animal was gone and she never approached it again. She learned a valuable lesson about the dangers of approaching strange animals.\n",
            "2. The Dangers of Curiosity\n",
            "Dora the Explorer is a beloved children’s show that teaches kids about the importance of being curious and exploring the world around them. However, Dora’s curiosity can sometimes lead her into dangerous situations.\n",
            "In one episode, Dora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download the model file\n",
        "hf_hub_download(repo_id=\"DSGT6AITest/webAccessibility\", filename=\"unsloth.Q8_0.gguf\", local_dir=\"./\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276,
          "referenced_widgets": [
            "198c4b5d825b4227b2fe2b8bf01a568d",
            "a1c008cd960b41fd979f3d03ac334480",
            "08c16194c4594c79865b7dcba079d7a2",
            "9d41d53febcb414696c6238d970287b7",
            "1a97cbfc71234e9297235d0dc42f0348",
            "79eede3476a641378684dd68e01749cd",
            "8d1f23f5b38b4040b77b71acc5018b62",
            "c8b1193e26f34bb4b1a315f7a25de32e",
            "ae1dbcb3fec5449eadcfe3de580f5243",
            "457bcada385c488cb81b8988f296ec72",
            "22ad31de617d44fb8a45582cfa7c9b7c"
          ]
        },
        "id": "d-zO9T5Bjj4Y",
        "outputId": "bf6c00f1-63ef-414b-825a-c9383ef93be3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2024.12.14)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q8_0.gguf:   0%|          | 0.00/8.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "198c4b5d825b4227b2fe2b8bf01a568d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'unsloth.Q8_0.gguf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def run_model(prompt):\n",
        "    # Define the command with the provided prompt\n",
        "    command = [\n",
        "        \"./build/bin/llama-cli\",\n",
        "        \"--hf-repo\", \"DSGT6AITest/webAccessibility\",\n",
        "        \"--hf-file\", \"unsloth.Q8_0.gguf\",\n",
        "        \"-p\", prompt,\n",
        "        \"--temp\", \"0.7\",\n",
        "        \"--top-k\", \"40\"\n",
        "    ]\n",
        "\n",
        "    # Run the command and capture the output\n",
        "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(\"Error running the model:\", result.stderr)\n",
        "    else:\n",
        "        print(\"Model Output:\")\n",
        "        print(result.stdout)\n",
        "\n",
        "# Example Usage with Deliberate Errors\n",
        "html_with_errors = \"\"\"\n",
        "<html>\n",
        "    <head>\n",
        "        <title>Test Page</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <img src=\"image.jpg\">\n",
        "        <p style=\"color: lightgray;\">This text is hard to read due to low contrast.</p>\n",
        "        <a href=\"#\">Click here</a>\n",
        "        <button>Submit</button>\n",
        "        <div role=\"button\" onclick=\"alert('Button clicked!')\">Not an actual button</div>\n",
        "    </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Pass the flawed HTML to the model\n",
        "run_model(f\"Analyze the following HTML for accessibility issues: {html_with_errors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W_PKGTWoMrg",
        "outputId": "b05a7d2b-7f75-4bd7-9a59-908e07adf21a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            "Analyze the following HTML for accessibility issues: \n",
            "<html>\n",
            "    <head>\n",
            "        <title>Test Page</title>\n",
            "    </head>\n",
            "    <body>\n",
            "        <img src=\"image.jpg\">\n",
            "        <p style=\"color: lightgray;\">This text is hard to read due to low contrast.</p>\n",
            "        <a href=\"#\">Click here</a>\n",
            "        <button>Submit</button>\n",
            "        <div role=\"button\" onclick=\"alert('Button clicked!')\">Not an actual button</div>\n",
            "    </body>\n",
            "</html>\n",
            "### Accessibility issues:\n",
            "The text in the paragraph element has low contrast, making it difficult for users with visual impairments to read. The image element lacks an alt attribute, which provides a textual description for non-visual users. The anchor element does not have a descriptive link text, which can be confusing for users who rely on screen readers. The button element is not a native button, but rather a div with a role of \"button\", which may not be recognized by assistive technology. \n",
            "### Solutions:\n",
            "To fix these issues, the text in the paragraph element can be made more readable by increasing the contrast between its color and background. The image element should include an alt attribute with a descriptive text, such as \"Image of a landscape\". The anchor element should have a descriptive link text, such as \"Click here to learn more\". The button element should be a native button element, rather than a div with a role of \"button\", to ensure it is recognized by assistive technology.\n",
            " [end of text]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def run_model(prompt):\n",
        "    # Define the command with the provided prompt\n",
        "    command = [\n",
        "        \"./build/bin/llama-cli\",\n",
        "        \"--hf-repo\", \"DSGT6AITest/webAccessibility\",\n",
        "        \"--hf-file\", \"unsloth.Q8_0.gguf\",\n",
        "        \"-p\", prompt,\n",
        "        \"--temp\", \"0.7\",\n",
        "        \"--top-k\", \"40\"\n",
        "    ]\n",
        "\n",
        "    # Run the command and capture the output\n",
        "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(\"Error running the model:\", result.stderr)\n",
        "    else:\n",
        "        print(\"Model Output:\")\n",
        "        print(result.stdout)\n",
        "\n",
        "# Example Usage with Deliberate Errors\n",
        "html_with_errors = \"\"\"\n",
        "<html>\n",
        "    <head>\n",
        "        <title>Test Page</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <img src=\"image.jpg\">\n",
        "        <p style=\"color: lightgray;\">This text is hard to read due to low contrast.</p>\n",
        "        <a href=\"#\">Click here</a>\n",
        "        <button>Submit</button>\n",
        "        <div role=\"button\" onclick=\"alert('Button clicked!')\">Not an actual button</div>\n",
        "    </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Pass the flawed HTML to the model\n",
        "run_model(f\"Analyze the following HTML for accessibility issues: {html_with_errors}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsfxTqYopo_f",
        "outputId": "aaffc003-77af-4e43-bd46-7a4e6d118722"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            "Analyze the following HTML for accessibility issues: \n",
            "<html>\n",
            "    <head>\n",
            "        <title>Test Page</title>\n",
            "    </head>\n",
            "    <body>\n",
            "        <img src=\"image.jpg\">\n",
            "        <p style=\"color: lightgray;\">This text is hard to read due to low contrast.</p>\n",
            "        <a href=\"#\">Click here</a>\n",
            "        <button>Submit</button>\n",
            "        <div role=\"button\" onclick=\"alert('Button clicked!')\">Not an actual button</div>\n",
            "    </body>\n",
            "</html>\n",
            "### Corrected HTML:\n",
            "<html>\n",
            "    <head>\n",
            "        <title>Test Page</title>\n",
            "    </head>\n",
            "    <body>\n",
            "        <img src=\"image.jpg\" alt=\"image description\">\n",
            "        <p style=\"color: #000000;\">This text is easier to read with increased contrast.</p>\n",
            "        <a href=\"#\">Click here</a>\n",
            "        <button>Submit</button>\n",
            "        <div role=\"button\" onclick=\"alert('Button clicked!')\">Not an actual button</div>\n",
            "    </body>\n",
            "</html>\n",
            "### Explanation:\n",
            "The corrected HTML ensures proper color contrast between text and background, provides alternative text for images, uses semantic HTML elements, and avoids using non-semantic elements like <div> as buttons.\n",
            " [end of text]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}